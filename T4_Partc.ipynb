{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhruv-Sharma01/ES335_Andrew_NG/blob/main/T4_Partc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TGH8ZQmKyvU7",
        "outputId": "07725af0-8597-435d-f987-0ea72da7bd44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.1.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.26 (from langchain_groq)\n",
            "  Downloading langchain_core-0.2.35-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading langsmith-0.1.106-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (24.1)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m983.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (2.0.7)\n",
            "Downloading langchain_groq-0.1.9-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.10.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.35-py3-none-any.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.106-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, groq, langchain-core, langchain_groq\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed groq-0.10.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.35 langchain_groq-0.1.9 langsmith-0.1.106 orjson-3.10.7 tenacity-8.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNzBmVL2x-yw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from langchain_groq.chat_models import ChatGroq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the Few-Shot prompting method using UCI-HAR dataset to predict the activities that you performed. Ensure that both your examples and test query undergo similar preprocessing. How did the model perform?"
      ],
      "metadata": {
        "id": "JgfsEO34XIaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('output_X.csv')\n",
        "df1 = pd.read_csv('output_Y.csv')\n",
        "df = pd.concat([df, df1], axis=1)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk-d9kLdE0RN",
        "outputId": "2a336fe5-542b-442a-f0e8-9ebc350ea0c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         accx    accy    accz           y_train\n",
            "0      0.9883 -0.0447  0.0562  WALKING_UPSTAIRS\n",
            "1      0.9854 -0.0481  0.0501  WALKING_UPSTAIRS\n",
            "2      0.9956 -0.0640  0.0007  WALKING_UPSTAIRS\n",
            "3      1.0039 -0.0706 -0.0000  WALKING_UPSTAIRS\n",
            "4      1.0130 -0.0623  0.0090  WALKING_UPSTAIRS\n",
            "...       ...     ...     ...               ...\n",
            "28282  1.2234 -0.2779  0.0058           WALKING\n",
            "28283  1.1741 -0.1879  0.0178           WALKING\n",
            "28284  0.9345  0.0812  0.0816           WALKING\n",
            "28285  0.8160  0.0433  0.1530           WALKING\n",
            "28286  0.8697  0.0125  0.1740           WALKING\n",
            "\n",
            "[28287 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the datasets\n",
        "df_X = pd.read_csv('output_X.csv')\n",
        "df_Y = pd.read_csv('output_Y.csv')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Optionally, verify the shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24qrvEGRXVD5",
        "outputId": "8bd9cc76-2624-4f5f-ebb2-3d40fb46f2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (22629, 3), y_train shape: (22629, 1)\n",
            "X_test shape: (5658, 3), y_test shape: (5658, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define a mapping from activity labels to human-readable activity names\n",
        "activity_labels = {\n",
        "    \"WALKING\": 1,\n",
        "    \"WALKING_UPSTAIRS\": 2,\n",
        "    \"WALKING_DOWNSTAIRS\": 3,\n",
        "    \"SITTING\": 4,\n",
        "    \"STANDING\": 5,\n",
        "    \"LAYING\": 6\n",
        "}\n",
        "\n",
        "# Sample a few random examples from the training set for few-shot learning\n",
        "few_shot_examples = []\n",
        "for _ in range(3):  # Choose how many examples you'd like to include\n",
        "    random_index = random.randint(0, len(X_train) - 1)\n",
        "    example_data = X_train.iloc[random_index].values\n",
        "    example_label = y_train.iloc[random_index].values[0]\n",
        "    few_shot_examples.append((example_data, activity_labels[example_label]))\n",
        "\n",
        "# Construct the few-shot input prompt\n",
        "few_shot_input_prompt = \"You are an AI trained to classify human activities based on accelerometer data.\\n\"\n",
        "few_shot_input_prompt += \"Here are some examples of how accelerometer data is classified into specific human activities:\\n\\n\"\n",
        "\n",
        "for i, (data, label) in enumerate(few_shot_examples):\n",
        "    few_shot_input_prompt += f\"Example {i+1}:\\n\"\n",
        "    few_shot_input_prompt += f\"Data: {list(data)}\\n\"\n",
        "    few_shot_input_prompt += f\"Label: {label}\\n\\n\"\n",
        "\n",
        "few_shot_input_prompt += \"Now, classify the following accelerometer data into the correct human activity.\\n\\n\"\n"
      ],
      "metadata": {
        "id": "wb4nL24VXZXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming the necessary imports and definitions are already in place\n",
        "llm_classification_predictions = []\n",
        "actual_activity_labels = []\n",
        "\n",
        "# Iterate over each row in X_test\n",
        "for index, row in X_test.iterrows():\n",
        "    # Prepare the data string from the row\n",
        "    test_data_string = list(row.values)\n",
        "\n",
        "    # Construct the classification query\n",
        "    classification_query = (f\"{few_shot_input_prompt}\\n\\n\"\n",
        "                            f\"Classify the following accelerometer data into the correct human activity.\\n\\n\"\n",
        "                            f\"Data: {test_data_string}\\n\\n\"\n",
        "                            \"Please provide the classification in the following format:\\n\"\n",
        "                            \"Label: <predicted_label>\")\n",
        "\n",
        "    # Initialize and use the language model\n",
        "    language_model_name = \"llama3-70b\"  # Model name\n",
        "    llm_model = ChatGroq(model=groq_models[language_model_name], api_key=Groq_token, temperature=0)\n",
        "    llm_output = llm_model.invoke(classification_query)\n",
        "\n",
        "    # Extract the LLM prediction\n",
        "    llm_predicted_label = extract_label(llm_output.content.strip())\n",
        "\n",
        "    # Store LLM prediction and actual label\n",
        "    llm_classification_predictions.append(llm_predicted_label)\n",
        "\n",
        "    # Use index alignment to get the correct label\n",
        "    actual_activity_label = y_test.iloc[index] if index < len(y_test) else None\n",
        "    actual_activity_labels.append(actual_activity_label.values[0] if actual_activity_label is not None else None)\n",
        "\n",
        "# Convert lists to numpy arrays for plotting\n",
        "predicted_values = np.array(llm_classification_predictions)\n",
        "actual_values = np.array(actual_activity_labels)\n",
        "\n",
        "# Step 3: Plot a Bar Graph\n",
        "\n",
        "# Get unique activity labels and their counts\n",
        "unique_labels, actual_counts = np.unique(actual_values, return_counts=True)\n",
        "_, predicted_counts = np.unique(predicted_values, return_counts=True)\n",
        "\n",
        "# Plotting the bar graph\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(unique_labels))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(index, actual_counts, bar_width, label='Actual', color='b')\n",
        "plt.bar(index + bar_width, predicted_counts, bar_width, label='Predicted', color='r')\n",
        "\n",
        "plt.xlabel('Activity Labels')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Comparison of Actual and Predicted Activity Labels')\n",
        "plt.xticks(index + bar_width / 2, [activity_labels[label] for label in unique_labels])\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8VLKPIbdZG9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u_0WJ2D5aRUi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}